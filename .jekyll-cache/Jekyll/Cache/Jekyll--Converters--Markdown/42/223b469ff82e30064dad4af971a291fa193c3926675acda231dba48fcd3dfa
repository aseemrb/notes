I"È<h2 id="definition">Definition</h2>
<p>For a matrix $A$, if there exists a vector $v$ such that $Av = \lambda v$ for some scalar $\lambda$, then $\lambda$ is called an eigenvalue and $v$ is called an eigenvector of $A$.</p>

<p>Naturally, eigenvalues are defined <span class="text-red-100">only for square matrices</span> because for non-square matrices, the transformation changes the dimensions of the resulting vector, hence $Av \neq \lambda v$ for any pair $(\lambda, v)$. Normally, for an $n\times n$ matrix, there are $n$ independent eigenvectors with $Ax_i = \lambda_ix_i$.</p>

<h2 id="what-are-eigenvectors">What are eigenvectors?</h2>
<p>One intuitive way to understand eigenvectors is to look at any matrix $A$ as a transformation being applied to a vector $v$. A transformation can rotate and/or scale a vector. Thereâ€™s a <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g">very nice video</a> about this by <a href="https://www.3blue1brown.com/about">Grant Sanderson</a> where he explains this idea with beautiful visuals. In essence, eigenvectors of a matrix (transformation) are those vectors that are not rotated, but only scaled without changing the direction. The magnitude of that scale is what we call the eigenvalue.</p>

<h2 id="properties">Properties</h2>
<p>The key property of eigenvectors can be seen by looking at $A^k$ for some positive integer $k$. Letâ€™s say $x$ is an eigenvector of $A$ with eigenvalue $\lambda$. Then we have</p>

\[A^kx = A^{k-1}(\lambda x) = \lambda (A^{k-1}x) = \dots = \lambda^kx.\]

<p>Thus, if $x$ is an eigenvector of $A$ with eigenvalue $\lambda$ then $x$ is also an eigenvector of $A^k$ with eigenvalue $\lambda^k$. This also tells us another key fact about matrices:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><span class="fs-4 text-green-100">Claim</span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">If 0 is an eigenvalue of $A$, then $A^{-1}$ does not exist. In other words, A is not invertible.</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><span class="fs-4 text-green-100">Proof</span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">An intuitive way to see this is to consider the eigenvector $x$ associated with eigenvalue $\lambda$. We have that $A^{-1}x = (1/\lambda) x$. So if $\lambda = 0$ then $A^{-1}$ is not defined.</td>
    </tr>
    <tr>
      <td style="text-align: left">More rigorously, if 0 is eigenvalue, it means that there exists a non-zero vector $x$ such that $Ax = 0$. Hence, the null space of $A$ is non-trivial (contains something other than the zero vector), and so $A$ is a dimension reducing transformation. This also means that $\det A = 0$. For more, check out the <a href="https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem">rank-nullity theorem</a>.</td>
    </tr>
  </tbody>
</table>

<p>The $n$ independent eigenvectors of $A$ form a basis for $\mathbb{R}^n$, so any vector $v$ can be expressed as a linear combination of the eigenvectors $x_i$:</p>

\[V = \sum_{i=1}^{n} c_ix_i.\]

<h2 id="similarity">Similarity</h2>

<p>A matrix $B$ is said to be <span class="text-red-100">similar to $A$</span> if
$B = M^{-1}AM$ for some invertible matrix $M$.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><span class="fs-4 text-green-100">Claim</span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">If $B$ is similar to $A$, then they have the same eigenvalues.</td>
    </tr>
  </tbody>
</table>

<table>
<thead><tr><th style="text-align: left"><span class="fs-4 text-green-100">Proof</span></th></tr></thead>
<tbody>
  <tr>
    <td>
    Let $y$ be an eigenvector of $B$ with eigenvalue $\lambda$. Then we have $By=\lambda y$. Taking $B=M^{-1}AM$ we see that
    $$\begin{align*}
    \lambda(My) &amp;= M\lambda y \\
    &amp;= MBy \\
    &amp;= MM^{-1}AMy \\
    &amp;= A(My)
    \end{align*}.$$

    Hence, $\lambda$ is also an eigenvalue of $A$ with eigenvector $My$.
    </td>
  </tr>
</tbody>
</table>

<p>This simple result is very useful in computation. For example, a software like MATLAB will use a sequence of matrices $M_1, M_2, \ldots, M_k$ and reduce $A$ to $B_k$ where $B_0 = A$ and $B_i = M_i^{-1}B_{i-1}M_i$ for $i&gt;0$. This sequence can be chosen carefully so as to <span class="text-red-100">make $B_k$ a diagonal matrix</span>, hence, the eigenvalues show up on the diagonal. This helps us calculate the eigenvalues of a matrix much faster.</p>

<p>For a matrix $A$, the <span class="text-red-100">eigenspace</span> associated with a set of eigenvalues is defined to be the subspace spanned by the eigenvectors associated with those eigenvalues.</p>

<p>Some obvious but important facts are as follows:</p>

<ul>
  <li>The sum of the eigenvalues of $A$ is ${\rm Trace}(A)$, which is the sum of the elements on the diagonal.</li>
  <li>The product of the eigenvalues of $A$ is $\det A$.</li>
  <li>In general, eigenvalues of $A + B$ or $AB$ cannot be inferred directly from eigenvalues of $A$ and $B$.</li>
</ul>

<p>The first two facts can be proved easily using the characteristic polynomial $P(\lambda) = \det(A - \lambda I) = 0$. We know that the sum of the roots of such a polynomial is determined by the coefficient of $\lambda^{n-1}$, $n$ being the degree of the polynomial. Also, the constant term in the polynomial determines the product of the roots.</p>

<h2 id="symmetric-matrices">Symmetric matrices</h2>
<p>If $S$ is a symmetric matrix, then</p>
<ul>
  <li>Eigenvalues of $S$ are real if $S$ is real.</li>
  <li>Eigenvectors of $S$ are orthogonal.</li>
  <li>We may have a full set of eigenvectors even if some eigenvalues are repeated. For example, consider the identity matrix. It has only one eigenvalue, $\lambda=1$, but every vector is an eigenvector.</li>
</ul>

<p>Let $S$ be an $n\times n$ symmetric matrix with eigenvalues $\lambda_1, \ldots, \lambda_n$. If we consider the matrix</p>

\[\Lambda = \begin{pmatrix}
    \lambda_1 &amp; \dots &amp; 0 \\
    \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; \dots &amp; \lambda_n
    \end{pmatrix},\]

<p>we see that $S$ and $\Lambda$ are <a href="#similarity">similar matrices</a>. This means that there must be an $M$ such that $S = M^{-1}\Lambda M$. It is not hard to see that $M$ is the eigenvector matrix (columns of $M$ are eigenvectors of $S$), and we have the <a href="../factorization/#spectral-decomposition">spectral decomposition</a> $S = Q\Lambda Q^{\rm T}$.</p>

<h2 id="general-matrices">General matrices</h2>

<p>For a general square matrix $A$ (may not be symmetric), we can factorize it as</p>

\[A = X\Lambda X^{-1}.\]

<p>where $X$ is the eigenvector matrix and $\Lambda$ it the diagonal eigenvalue matrix. This factorization is another way to look at the fact that the eigenvectors of exponents of $A$ are the same as that of $A$, with corresponding exponentiated eigenvalues. It is only when $A$ is symmetric that we can use $X^{-1} = X^{\rm T}$, since the eigenvectors are orthogonal in that case.</p>
:ET